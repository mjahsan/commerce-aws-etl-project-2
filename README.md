ğŸš€ EVENT-DRIVEN AWS DATA PIPELINE (S3 â†’ LAMBDA â†’ GLUE â†’ S3)


ğŸ“Œ OVERVIEW
This project implements an event-driven data pipeline on AWS that ingests raw JSON data, validates files, performs transformations using Spark SQL, and produces analytics-ready Parquet output using a Bronzeâ€“Silverâ€“Gold lakehouse pattern.
The pipeline simulates a real-world commerce data processing workflow using fully managed AWS services and production-oriented design practices.


ğŸ—ï¸ ARCHITECTURE:

ğŸ”„ Flow:
ğŸ“¥ Raw JSON files are uploaded to the Bronze layer in Amazon S3
âš¡ An S3 event triggers an AWS Lambda function
ğŸ§ª Lambda performs file-level validation (format & naming)
ğŸ“¤ Valid files are promoted to the Silver layer
ğŸš¦ Once all required datasets are present, Lambda triggers an AWS Glue Spark job
ğŸ”¥ Glue performs transformations using Spark SQL
ğŸ“Š Final aggregated output is written to the Gold layer in Parquet format
ğŸ–¼ï¸ Refer to: architecture/pipeline_architecture.png


ğŸ—‚ï¸ DATA MODELS:

ğŸ“ Input Datasets:
-> users.json
-> events.json
-> orders.json
Each dataset contains ~1 million records, generated using Python to test scalability and performance.


ğŸ”§ TRANSFORMATIONS:

The AWS Glue job:
âœ… Handles null values
ğŸ”— Joins users, events, and orders datasets
ğŸ“ˆ Aggregates user-level activity metrics
ğŸ§¾ Produces a single user_activity dataset
ğŸ’¾ Writes output as Parquet to the Gold layer
All transformations are implemented using Spark SQL for clarity, performance, and maintainability.


ğŸ“ REPOSITORY STRUCTURE:

commerce-data-pipeline/
â”‚
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ architecture/
â”‚   â””â”€â”€ pipeline_architecture.png
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_data/
â”‚       â”œâ”€â”€ users.json
â”‚       â”œâ”€â”€ events.json
â”‚       â””â”€â”€ orders.json
â”‚
â”œâ”€â”€ lambda/
â”‚   â””â”€â”€ bronze_to_silver/
â”‚       â”œâ”€â”€ lambda_function.py
â”‚       â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ glue/
â”‚   â””â”€â”€ silver_to_gold/
â”‚       â””â”€â”€ user_activity_job.py
â”‚
â”œâ”€â”€ iam/
â”‚   â”œâ”€â”€ lambda_execution_policy.json
â”‚   â””â”€â”€ glue_execution_policy.json
â”‚
â”œâ”€â”€ kms/
â”‚   â””â”€â”€ key_policy.json
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ generate_sample_data.py
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ pipeline_config.yaml
â”‚
â””â”€â”€ .gitignore


ğŸ” SECURITY:

ğŸ”‘ IAM execution roles created for Lambda and Glue
ğŸ“œ Least-privilege policies defined in iam/
ğŸ”’ S3 data encrypted using AWS KMS
ğŸ›¡ï¸ KMS key policies included for reference
âŒ No credentials stored in the repository
This project follows security best practices aligned with production-grade AWS pipelines.


â–¶ï¸ HOW TO RUN (HIGH LEVEL):

ğŸ§± Deploy IAM roles and policies
ğŸª£ Create an S3 bucket with Bronze / Silver / Gold structure
âš¡ Create a Lambda function for Bronze â†’ Silver validation with an S3 upload trigger
ğŸ“¥ Upload sample data to the Bronze layer
ğŸ” Lambda automatically validates and promotes data
ğŸ”¥ Glue job is triggered automatically
ğŸ“Š Final Parquet output is available in the Gold layer


ğŸ› ï¸ TECH STACK:

â˜ï¸ Amazon S3
âš¡ AWS Lambda (Python)
ğŸ”¥ AWS Glue (PySpark, Spark SQL)
ğŸ” IAM
ğŸ”‘ AWS KMS
ğŸ“¡ AWS CloudWatch


ğŸ¯ WHAT THIS PROJECT DEMONSTRATES:

âš¡ Event-driven data ingestion
ğŸ›ï¸ Lakehouse design principles (Bronze / Silver / Gold)
ğŸ”¥ Spark-based data transformations
ğŸ” Cloud security fundamentals
ğŸ§  Production-oriented pipeline structuring