# ğŸš€ EVENT-DRIVEN AWS DATA PIPELINE (S3 â†’ LAMBDA â†’ GLUE â†’ S3)

---

## ğŸ“Œ OVERVIEW
This project implements an event-driven data pipeline on AWS that ingests raw JSON data, validates files, performs transformations using Spark SQL, and produces analytics-ready Parquet output using a Bronzeâ€“Silverâ€“Gold lakehouse pattern.
The pipeline simulates a real-world commerce data processing workflow using fully managed AWS services and production-oriented design practices.

---

## ğŸ—ï¸ ARCHITECTURE:

### ğŸ”„ Flow:
- Raw JSON files are uploaded to the Bronze layer in Amazon S3
- An S3 event triggers an AWS Lambda function
- Lambda performs file-level validation (format & naming)
- Valid files are promoted to the Silver layer
- Once all required datasets are present, Lambda triggers an AWS Glue Spark job
- Glue performs transformations using Spark SQL
- Final aggregated output is written to the Gold layer in Parquet format

Refer to: architecture/pipeline_architecture.png

---

## ğŸ“Š DATA MODELS:

### Input Datasets:
- users.json
- events.json
- orders.json

Each dataset contains ~1 million records, generated using Python to test scalability and performance.

---

## ğŸ”„ TRANSFORMATIONS:

### The AWS Lambda job:
- Validates the incoming files from bronze/raw layer for file types and empty files
- Handles the empty and non-relevant files by skipping them
- Transfers the approved files to the silver layer
- Triggers the downstream Glue job when all the files have been transferred to the silver layer
  
All transformations are implemented using Python

### The AWS Glue job:
- Handles null values
- Joins users, events, and orders datasets
- Aggregates user-level activity metrics
- Produces a single user_activity dataset
- Writes output as Parquet to the Gold layer
  
All transformations are implemented using Spark SQL for clarity, performance, and maintainability.

---

## ğŸ“ REPOSITORY STRUCTURE:

```text
commerce-data-pipeline/
â”‚
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ architecture/
â”‚   â””â”€â”€ pipeline_architecture.png
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_data/
â”‚       â”œâ”€â”€ users.json
â”‚       â”œâ”€â”€ events.json
â”‚       â””â”€â”€ orders.json
â”‚
â”œâ”€â”€ lambda/
â”‚   â””â”€â”€ bronze_to_silver/
â”‚       â””â”€â”€ commerce_data_validation.py
â”‚       
â”‚
â”œâ”€â”€ glue/
â”‚   â””â”€â”€ silver_to_gold/
â”‚       â””â”€â”€ user_activity_job.py
â”‚
â”œâ”€â”€ iam/
â”‚   â”œâ”€â”€ lambda_execution_policy.json
â”‚   â””â”€â”€ glue_execution_policy.json
â”‚
â”œâ”€â”€ kms/
â”‚   â””â”€â”€ key_policy.json
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ generate_sample_data.py
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ pipeline_config.yaml
â”‚
â””â”€â”€ .gitignore
```
---

## ğŸ” SECURITY:

- IAM execution roles created for Lambda and Glue
- Least-privilege policies defined in iam/
- S3 data encrypted using AWS KMS
- KMS key policies included for reference
- No credentials stored in the repository
  
This project follows security best practices aligned with production-grade AWS pipelines.

---

## â–¶ï¸ HOW TO RUN (HIGH LEVEL):

- Deploy IAM roles and policies
- Create an S3 bucket with Bronze / Silver / Gold structure
- Create a Lambda function for Bronze â†’ Silver validation with an S3 upload trigger
- Upload sample data to the Bronze layer
- Lambda automatically validates and promotes data
- Glue job is triggered automatically
- Final Parquet output is available in the Gold layer

---

## ğŸ› ï¸ TECH STACK:

- Amazon S3
- AWS Lambda (Python)
- AWS Glue (PySpark, Spark SQL)
- IAM
- AWS KMS
- AWS CloudWatch

---

## ğŸ¯ WHAT THIS PROJECT DEMONSTRATES:

- Event-driven data ingestion
- Lakehouse design principles (Bronze / Silver / Gold)
- Spark-based data transformations
- Cloud security fundamentals
- Production-oriented pipeline structuring
